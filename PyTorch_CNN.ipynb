{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.functional import elu\n",
    "from braindecode.torch_ext.modules import Expression, AvgPool2dWithConv\n",
    "from braindecode.torch_ext.functions import identity\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "\n",
    "\n",
    "class Deep4Net(object):\n",
    "    \"\"\"\n",
    "    Deep ConvNet model from [1]_.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., \n",
    "       Glasstetter, M., Eggensperger, K., Tangermann, M., ... & Ball, T. (2017).\n",
    "       Deep learning with convolutional neural networks for EEG decoding and\n",
    "       visualization.\n",
    "       arXiv preprint arXiv:1703.05051.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans,\n",
    "                 n_classes,\n",
    "                 input_time_length,\n",
    "                 final_conv_length,\n",
    "                 n_filters_time=25,\n",
    "                 n_filters_spat=25,\n",
    "                 filter_time_length=10,\n",
    "                 pool_time_length=3,\n",
    "                 pool_time_stride=3,\n",
    "                 n_filters_2=50,\n",
    "                 filter_length_2=10,\n",
    "                 n_filters_3=100,\n",
    "                 filter_length_3=10,\n",
    "                 n_filters_4=200,\n",
    "                 filter_length_4=10,\n",
    "                 first_nonlin=elu,\n",
    "                 first_pool_mode='max',\n",
    "                 first_pool_nonlin=identity,\n",
    "                 later_nonlin=elu,\n",
    "                 later_pool_mode='max',\n",
    "                 later_pool_nonlin=identity,\n",
    "                 drop_prob=0.5,\n",
    "                 double_time_convs=False,\n",
    "                 split_first_layer=True,\n",
    "                 batch_norm=True,\n",
    "                 batch_norm_alpha=0.1,\n",
    "                 stride_before_pool=False):\n",
    "        if final_conv_length == 'auto':\n",
    "            assert input_time_length is not None\n",
    "\n",
    "        self.__dict__.update(locals())\n",
    "        del self.self\n",
    "\n",
    "    def create_network(self):\n",
    "        if self.stride_before_pool:\n",
    "            conv_stride = self.pool_time_stride\n",
    "            pool_stride = 1\n",
    "        else:\n",
    "            conv_stride = 1\n",
    "            pool_stride = self.pool_time_stride\n",
    "        pool_class_dict = dict(max=nn.MaxPool2d, mean=AvgPool2dWithConv)\n",
    "        first_pool_class = pool_class_dict[self.first_pool_mode]\n",
    "        later_pool_class = pool_class_dict[self.later_pool_mode]\n",
    "        model = nn.Sequential()\n",
    "        if self.split_first_layer:\n",
    "            model.add_module('dimshuffle', Expression(_transpose_time_to_spat))\n",
    "            model.add_module('conv_time', nn.Conv2d(1, self.n_filters_time,\n",
    "                                                    (\n",
    "                                                    self.filter_time_length, 1),\n",
    "                                                    stride=1, ))\n",
    "            model.add_module('conv_spat',\n",
    "                             nn.Conv2d(self.n_filters_time, self.n_filters_spat,\n",
    "                                       (1, self.in_chans),\n",
    "                                       stride=(conv_stride, 1),\n",
    "                                       bias=not self.batch_norm))\n",
    "            n_filters_conv = self.n_filters_spat\n",
    "        else:\n",
    "            model.add_module('conv_time',\n",
    "                             nn.Conv2d(self.in_chans, self.n_filters_time,\n",
    "                                       (self.filter_time_length, 1),\n",
    "                                       stride=(conv_stride, 1),\n",
    "                                       bias=not self.batch_norm))\n",
    "            n_filters_conv = self.n_filters_time\n",
    "        if self.batch_norm:\n",
    "            model.add_module('bnorm',\n",
    "                             nn.BatchNorm2d(n_filters_conv,\n",
    "                                            momentum=self.batch_norm_alpha,\n",
    "                                            affine=True,\n",
    "                                            eps=1e-5),)\n",
    "        model.add_module('conv_nonlin', Expression(self.first_nonlin))\n",
    "        model.add_module('pool',\n",
    "                         first_pool_class(\n",
    "                             kernel_size=(self.pool_time_length, 1),\n",
    "                             stride=(pool_stride, 1)))\n",
    "        model.add_module('pool_nonlin', Expression(self.first_pool_nonlin))\n",
    "\n",
    "        def add_conv_pool_block(model, n_filters_before,\n",
    "                                n_filters, filter_length, block_nr):\n",
    "            suffix = '_{:d}'.format(block_nr)\n",
    "            model.add_module('drop' + suffix,\n",
    "                             nn.Dropout(p=self.drop_prob))\n",
    "            model.add_module('conv' + suffix.format(block_nr),\n",
    "                             nn.Conv2d(n_filters_before, n_filters,\n",
    "                                       (filter_length, 1),\n",
    "                                       stride=(conv_stride, 1),\n",
    "                                       bias=not self.batch_norm))\n",
    "            if self.batch_norm:\n",
    "                model.add_module('bnorm' + suffix,\n",
    "                             nn.BatchNorm2d(n_filters,\n",
    "                                            momentum=self.batch_norm_alpha,\n",
    "                                            affine=True,\n",
    "                                            eps=1e-5))\n",
    "            model.add_module('nonlin' + suffix,\n",
    "                             Expression(self.later_nonlin))\n",
    "\n",
    "            model.add_module('pool' + suffix,\n",
    "                             later_pool_class(\n",
    "                                 kernel_size=(self.pool_time_length, 1),\n",
    "                                 stride=(pool_stride, 1)))\n",
    "            model.add_module('pool_nonlin' + suffix,\n",
    "                             Expression(self.later_pool_nonlin))\n",
    "\n",
    "        add_conv_pool_block(model, n_filters_conv, self.n_filters_2,\n",
    "                            self.filter_length_2, 2)\n",
    "        add_conv_pool_block(model, self.n_filters_2, self.n_filters_3,\n",
    "                            self.filter_length_3, 3)\n",
    "        add_conv_pool_block(model, self.n_filters_3, self.n_filters_4,\n",
    "                            self.filter_length_4, 4)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        if self.final_conv_length == 'auto':\n",
    "            out = model(np_to_var(np.ones(\n",
    "                (1, self.in_chans, self.input_time_length,1),\n",
    "                dtype=np.float32)))\n",
    "            n_out_time = out.cpu().data.numpy().shape[2]\n",
    "            self.final_conv_length = n_out_time\n",
    "        model.add_module('conv_classifier',\n",
    "                             nn.Conv2d(self.n_filters_4, self.n_classes,\n",
    "                                       (self.final_conv_length, 1), bias=True))\n",
    "        model.add_module('softmax', nn.LogSoftmax())\n",
    "        model.add_module('squeeze',  Expression(_squeeze_final_output))\n",
    "\n",
    "        # Initialization, xavier is same as in our paper...\n",
    "        # was default from lasagne\n",
    "        init.xavier_uniform(model.conv_time.weight, gain=1)\n",
    "        # maybe no bias in case of no split layer and batch norm\n",
    "        if self.split_first_layer or (not self.batch_norm):\n",
    "            init.constant(model.conv_time.bias, 0)\n",
    "        if self.split_first_layer:\n",
    "            init.xavier_uniform(model.conv_spat.weight, gain=1)\n",
    "            if not self.batch_norm:\n",
    "                init.constant(model.conv_spat.bias, 0)\n",
    "        if self.batch_norm:\n",
    "            init.constant(model.bnorm.weight, 1)\n",
    "            init.constant(model.bnorm.bias, 0)\n",
    "        param_dict = dict(list(model.named_parameters()))\n",
    "        for block_nr in range(2,5):\n",
    "            conv_weight = param_dict['conv_{:d}.weight'.format(block_nr)]\n",
    "            init.xavier_uniform(conv_weight, gain=1)\n",
    "            if not self.batch_norm:\n",
    "                conv_bias = param_dict['conv_{:d}.bias'.format(block_nr)]\n",
    "                init.constant(conv_bias, 0)\n",
    "            else:\n",
    "                bnorm_weight = param_dict['bnorm_{:d}.weight'.format(block_nr)]\n",
    "                bnorm_bias = param_dict['bnorm_{:d}.bias'.format(block_nr)]\n",
    "                init.constant(bnorm_weight, 1)\n",
    "                init.constant(bnorm_bias, 0)\n",
    "\n",
    "        init.xavier_uniform(model.conv_classifier.weight, gain=1)\n",
    "        init.constant(model.conv_classifier.bias, 0)\n",
    "\n",
    "        # Start in eval mode\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "\n",
    "# remove empty dim at end and potentially remove empty time dim\n",
    "# do not just use squeeze as we never want to remove first dim\n",
    "def _squeeze_final_output(x):\n",
    "    assert x.size()[3] == 1\n",
    "    x = x[:,:,:,0]\n",
    "    if x.size()[2] == 1:\n",
    "        x = x[:,:,0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def _transpose_time_to_spat(x):\n",
    "    return x.permute(0, 3, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d (1, 25, kernel_size=(10, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d (25, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (conv_nonlin): Expression(expression=elu)\n",
      "  (pool): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), dilation=(1, 1))\n",
      "  (pool_nonlin): Expression(expression=identity)\n",
      "  (drop_2): Dropout(p=0.5)\n",
      "  (conv_2): Conv2d (25, 50, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (nonlin_2): Expression(expression=elu)\n",
      "  (pool_2): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), dilation=(1, 1))\n",
      "  (pool_nonlin_2): Expression(expression=identity)\n",
      "  (drop_3): Dropout(p=0.5)\n",
      "  (conv_3): Conv2d (50, 100, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (nonlin_3): Expression(expression=elu)\n",
      "  (pool_3): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), dilation=(1, 1))\n",
      "  (pool_nonlin_3): Expression(expression=identity)\n",
      "  (drop_4): Dropout(p=0.5)\n",
      "  (conv_4): Conv2d (100, 200, kernel_size=(10, 1), stride=(1, 1), bias=False)\n",
      "  (bnorm_4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (nonlin_4): Expression(expression=elu)\n",
      "  (pool_4): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), dilation=(1, 1))\n",
      "  (pool_nonlin_4): Expression(expression=identity)\n",
      "  (conv_classifier): Conv2d (200, 60, kernel_size=(10, 1), stride=(1, 1))\n",
      "  (softmax): LogSoftmax()\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight[25, 1, 10, 1], so expected input[1, 500, 60, 1] to have 1 channels, but got 500 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e47a3b615d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dir(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight[25, 1, 10, 1], so expected input[1, 500, 60, 1] to have 1 channels, but got 500 channels instead"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "net = Deep4Net(1, 60, 500, 10)\n",
    "model = net.create_network()\n",
    "# dir(model)\n",
    "print(model)\n",
    "summary(model, (1, 60, 500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=60, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]           15010\n",
      "            Conv2d-2             [-1, 20, 8, 8]            5020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]           16050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 36590\n",
      "Trainable params: 36590\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (60, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
