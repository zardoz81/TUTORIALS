{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hermesdt/reinforcement-learning/blob/master/ppo/cartpole_ppo_online.ipynb\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, input): \n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor module, categorical actions only\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "# Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b095950>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)\n",
    "adam_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)\n",
    "\n",
    "def policy_loss(old_log_prob, log_prob, advantage, eps):\n",
    "    ratio = (log_prob - old_log_prob).exp()\n",
    "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
    "    \n",
    "    m = torch.min(ratio*advantage, clipped)\n",
    "    return -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2526ae04f355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m             w.add_histogram(\"gradients/critic\",\n\u001b[1;32m     48\u001b[0m                              torch.cat([p.data.view(-1) for p in critic.parameters()]), global_step=s)\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0madam_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprev_prob_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "gamma = 0.98\n",
    "eps = 0.2\n",
    "w = tensorboard.SummaryWriter()\n",
    "s = 0\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "for i in range(800):\n",
    "    prev_prob_act = None\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        s += 1\n",
    "        probs = actor(t(state))\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        prob_act = dist.log_prob(action)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action.detach().data.numpy())\n",
    "        advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))\n",
    "        \n",
    "        w.add_scalar(\"loss/advantage\", advantage, global_step=s)\n",
    "        w.add_scalar(\"actions/action_0_prob\", dist.probs[0], global_step=s)\n",
    "        w.add_scalar(\"actions/action_1_prob\", dist.probs[1], global_step=s)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if prev_prob_act:\n",
    "            actor_loss = policy_loss(prev_prob_act.detach(), prob_act, advantage.detach(), eps)\n",
    "            w.add_scalar(\"loss/actor_loss\", actor_loss, global_step=s)\n",
    "            adam_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            # clip_grad_norm_(adam_actor, max_grad_norm)\n",
    "            w.add_histogram(\"gradients/actor\",\n",
    "                             torch.cat([p.grad.view(-1) for p in actor.parameters()]), global_step=s)\n",
    "            adam_actor.step()\n",
    "\n",
    "            critic_loss = advantage.pow(2).mean()\n",
    "            w.add_scalar(\"loss/critic_loss\", critic_loss, global_step=s)\n",
    "            adam_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            # clip_grad_norm_(adam_critic, max_grad_norm)\n",
    "            w.add_histogram(\"gradients/critic\",\n",
    "                             torch.cat([p.data.view(-1) for p in critic.parameters()]), global_step=s)\n",
    "            adam_critic.step()\n",
    "        \n",
    "        prev_prob_act = prob_act\n",
    "    \n",
    "    w.add_scalar(\"reward/episode_reward\", total_reward, global_step=i)\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
