{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform as im_tf\n",
    "from LossHistory import LossHistory\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# make sure you don't hog all the video memory\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# os.environ[\"CUDAa_VISIBLE_DEVICES\"]=\"2\"\n",
    "###################################\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Conv2D, Flatten, Reshape, Lambda, MaxPooling2D, BatchNormalization\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gym\n",
    "import pickle\n",
    "\n",
    "# def RGB2gray(img):\n",
    "#     R, G, B = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "#     return 1/3 * R + 1/3 * G + 1/3 * B\n",
    "\n",
    "def prepro(o, image_size=[80, 80]):\n",
    "    y = 0.2126 * o[:, :, 0] + 0.7152 * o[:, :, 1] + 0.0722 * o[:, :, 2]\n",
    "    y = y.astype(np.uint8)\n",
    "    resized = im_tf.resize(y, image_size, mode='constant')\n",
    "    return np.expand_dims(resized.astype(np.float32), axis=2).ravel()\n",
    "\n",
    "# def preprocess(I):\n",
    "#     \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "#     I = I[35:195] # crop\n",
    "#     I = I[::2,::2,0] # downsample by factor of 2\n",
    "#     I[I == 144] = 0 # erase background (background type 1)\n",
    "#     I[I == 109] = 0 # erase background (background type 2)\n",
    "#     I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "#     return I.astype(np.float).ravel()\n",
    "\n",
    "class Agent_PG:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"Pong-v0\")\n",
    "        self.S = None\n",
    "        self.mean = 0.\n",
    "        self.std = 1.\n",
    "        self.nda = []\n",
    "        self.batch_size = 32\n",
    "        self.__build_network()\n",
    "        self.__build_train_fn()\n",
    "        self.__init_game_setting()\n",
    "        \n",
    "    def __init_game_setting(self):\n",
    "        self.observation = self.env.reset()\n",
    "        pass\n",
    "    \n",
    "    def __build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((80, 80, 1), input_shape=(6400,)))\n",
    "        model.add(Conv2D(16, (8, 8), strides = (4, 4), activation='relu', kernel_initializer='lecun_uniform'))\n",
    "        model.add(Conv2D(32, (4, 4), strides = (2, 2), activation='relu', kernel_initializer='lecun_uniform'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='lecun_uniform'))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "        \n",
    "    def __build_train_fn(self):\n",
    "        # def loss(discount_r):\n",
    "        #     def f(y_true, y_pred):\n",
    "        #         action_prob = K.sum(y_true*y_pred, axis=1)\n",
    "        #         action_prob = K.log(action_prob)\n",
    "        #         policy_loss = -K.sum(discount_r) * K.mean(action_prob)\n",
    "        #         policy_loss = K.print_tensor(policy_loss)\n",
    "        #         return policy_loss\n",
    "        #     return f\n",
    "        # discount_reward_ = Input(shape=(1,))\n",
    "        # state = Input(shape=(6400,))\n",
    "        # pi_action = self.model(state)\n",
    "        # model = Model([state, discount_reward_], pi_action)\n",
    "        # adam = Adam(lr=1e-4)\n",
    "        # rmsprop = RMSprop(lr=1e-4 ,clipnorm=1) #10\n",
    "        # model.compile(optimizer=rmsprop, loss=loss(discount_reward_))\n",
    "        \n",
    "        action_prob_placeholder = self.model.output\n",
    "        action_onehot_placeholder = K.placeholder(shape=(None, 2))\n",
    "        discount_reward_placeholder = K.placeholder(shape=(None,))\n",
    "        action_prob = K.sum(action_prob_placeholder * action_onehot_placeholder, axis=1)\n",
    "        log_action_prob = K.log(action_prob)\n",
    "        loss = - log_action_prob * discount_reward_placeholder\n",
    "        loss = K.sum(loss)\n",
    "        adam = Adam(lr=1e-4)\n",
    "#         rmsprop = RMSprop(lr=1e-4, decay=0.99)\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,\n",
    "                                   loss=loss)\n",
    "        self.train_fn = K.function(inputs=[self.model.input,\n",
    "                                           action_onehot_placeholder,\n",
    "                                           discount_reward_placeholder],\n",
    "                                   outputs=[loss],\n",
    "                                   updates=updates)\n",
    "        # self.update_model = model\n",
    "\n",
    "    def fit(self, S, A, discount_reward):\n",
    "        action_onehot = to_categorical(A.reshape(-1), num_classes=2)\n",
    "        print(['{0:.2f}'.format(el) for el in np.mean(action_onehot, axis=0).tolist()])\n",
    "        loss = self.train_fn([S.reshape(S.shape[0], -1), action_onehot, discount_reward])\n",
    "        return loss\n",
    "    \n",
    "    def run_episode(self,i):  ####### playing one episode\n",
    "        state = self.observation\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        # S, A, R , sample_R= [], [], [], []\n",
    "        S = np.zeros([10000, 6400])\n",
    "        A = np.zeros([10000,])\n",
    "        R = np.zeros([10000,])\n",
    "        j = 0\n",
    "        while not done:\n",
    "            action = self.make_action(state, test=False)\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            S[j] = self.S\n",
    "            A[j] = 0 if action == 2 else 1\n",
    "            R[j] = reward\n",
    "            j = j + 1\n",
    "        self.nda = sum(A)/j\n",
    "        if i==0 and (0.45> sum(A)/j or sum(A)/j >0.55): exit()\n",
    "\n",
    "        def compute_discounted_R(R, discount_rate=.99):\n",
    "            discounted_r = np.zeros_like(R, dtype=np.float32)\n",
    "            running_add = 0\n",
    "            for t in reversed(range(R.shape[0])):\n",
    "                if R[t] != 0: running_add = 0\n",
    "                running_add = running_add * discount_rate + R[t]\n",
    "                discounted_r[t] = running_add\n",
    "            discounted_r = (discounted_r-discounted_r.mean()) / (discounted_r.std()+0.00001)\n",
    "            return discounted_r\n",
    "        \n",
    "        RR = R[:j]\n",
    "        RR = compute_discounted_R(RR)\n",
    "        return S[:j], A[:j], RR-0.01, episode_reward\n",
    "#         return S[:j], A[:j], RR, episode_reward\n",
    "\n",
    "    def train(self, n_episodes):\n",
    "        reward_history = []\n",
    "        for i in range(n_episodes):\n",
    "            self.__init_game_setting()\n",
    "            S, A, discount_reward, episode_reward = self.run_episode(i)\n",
    "            loss = self.fit(S, A, discount_reward)\n",
    "\n",
    "            ########### print and save\n",
    "            print('episode:', i, 'episode reward:', episode_reward)\n",
    "            with open(\"log_PG_2.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"episode \" + str(i) + \"\\t\" +\n",
    "                             \"loss \" + str(loss) + \"\\t\" +\n",
    "                             \" episode reward \" + str(episode_reward) + \"\\t\" +\n",
    "                             \" number of down act \" + str(self.nda) + \"\\t\" +\n",
    "                             \" game_len \" + str(len(discount_reward)) + \"\\t\" +\n",
    "                             \"\\n\")\n",
    "            reward_history.append(episode_reward)\n",
    "            self.model.save_weights('pong_pg111_weights.h5')\n",
    "#             if episode_reward > 5:\n",
    "#                 self.model.save_weights('pong_pg_weights111_' + str(episode_reward) + '_' + str(i) + '.h5')\n",
    "\n",
    "    def make_action(self, observation, test=True):\n",
    "        prev_observation = observation\n",
    "        observation = prepro(observation - self.observation)\n",
    "        pi_action = self.model.predict(observation.reshape(1,-1))\n",
    "        pi_action = np.squeeze(pi_action, axis=0)\n",
    "        if test:\n",
    "            action = pi_action.argmax()\n",
    "        else:\n",
    "            action = np.random.choice(2, p=pi_action)\n",
    "        self.observation = prev_observation\n",
    "        self.S = observation\n",
    "        return 2 if action == 0 else 3\n",
    "\n",
    "agent = Agent_PG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 15000\n",
    "# agent.model.load_weights('pong_pg111_weights.h5')\n",
    "agent.train(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
