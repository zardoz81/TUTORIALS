{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram model\n",
    "We’re going to train a fully connected neural network with a sigle hiddel layer to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby (within a certain range, e.g. 5 words back and 5 words ahead of the input word) and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "\n",
    "The input words are one-hot encoded (i.e. for a vocabulary of size 1000, each word is encoded as a sparse vector of size 1000 with only one non-zero element (1) at the position corresponding to some word)\n",
    "\n",
    "So a training sample (input-label pair) for such a model looks like this:\n",
    "\n",
    "**Text:**\n",
    "_A mother was washing a frame on **nice** and sunny day with some smelly chemical cleaning agent._\n",
    "\n",
    "1) We pick the word _nice_, one-hot encode it (or just look it up from a table of one-hot encodings\n",
    "\n",
    "2) We randomly sample a word from within 5 words before and after this word _nice_. Say it happens to be _sunny_. \n",
    "\n",
    "3) We may as well package these samples into batches for more efficient training.\n",
    "\n",
    "4) We train the model on a bunch of samples (batches of samples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/Screen Shot 2019-07-18 at 14.02.05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of training words $w_1, w_2, w_3, . . . , w_T$ , the objective of the Skip-gram model is to maximize the _average_ log probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c<j<c, c\\neq0}\\log p(w_t|w_{t+j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the network we just take the weights of the hidden layer, and these weights are our embeddings. The weight matrix has size $W \\times M$, where $W$ is the vocabulary size and $M$ is the size of the embeddings.\n",
    "![title](img/Screen Shot 2019-07-18 at 14.22.22.png)\n",
    "![title](img/Screen Shot 2019-07-18 at 14.22.08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for more:\n",
    "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "\n",
    "https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n",
    "\n",
    "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
